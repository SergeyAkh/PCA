{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ddc7c33-8830-4470-a675-8a167ae265a8",
   "metadata": {},
   "source": [
    "# PCA derivation witn Lagrange multipliers and usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7551cd78-9bff-4ce8-8c21-6115b3d4f74e",
   "metadata": {},
   "source": [
    "# Outline\n",
    "- [ 1 - Theory behind PCA](#1)\n",
    "- [ 2 - Import Data and brief EDA](#2)\n",
    "- [ 3 - Feature selection and train custom model](#3)\n",
    "- [ 4 - Training sklearm model and fine tuning](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a341910-e6f9-4a62-a04e-c9033200fe1b",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - Theory behind PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b52f1d7c-1ead-4456-b525-0a9d83e04611",
   "metadata": {},
   "source": [
    "<br>The goal of PCA is to find a set of orthogonal vectors (principal components) that capture the maximum variance in the data. We want to find these vectors in order of decreasing variance explained.\n",
    "<br>The Principal Component Analysis (PCA) approach using Lagrange multipliers is an elegant mathematical method to derive the principal components. This approach provides a direct way to find the directions of maximum variance in the data. Let's break down this process step by step!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d63c4b99-eb20-4241-94d6-0a5b09f9f744",
   "metadata": {},
   "source": [
    "1.\tMatrix Representation of Data: First, let's consider how data is represented in matrix form. Suppose we have n observations of p variables. We can represent this data as a matrix X:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3e23840-23b5-4fe4-81d2-aefb5f40af32",
   "metadata": {},
   "source": [
    "$$\n",
    "X = [ [x₁₁, x₁₂, ..., x₁ₚ], [x₂₁, x₂₂, ..., x₂ₚ], ... [xₙ₁, xₙ₂, ..., xₙₚ] ]\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2227fd60-5fd5-4627-9296-c1de51919b7c",
   "metadata": {},
   "source": [
    "Where xᵢⱼ represents the value of the j-th variable for the i-th observation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "619af0eb-56ce-497a-9d90-46cfae397c3d",
   "metadata": {},
   "source": [
    "2.\tVariance-Covariance Matrix: The variance of this multivariate data is typically represented by the variance-covariance matrix, often denoted as Σ (sigma). This matrix is a p × p symmetric matrix where:\n",
    "•\tThe diagonal elements (i = j) represent the variances of individual variables.\n",
    "•\tThe off-diagonal elements (i ≠ j) represent the covariances between pairs of variables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a578902-f118-4605-94c9-7c39a1ed4c83",
   "metadata": {},
   "source": [
    "$$\n",
    "Σ = [ [σ₁₁, σ₁₂, ..., σ₁ₚ], [σ₂₁, σ₂₂, ..., σ₂ₚ], ... [σₚ₁, σₚ₂, ..., σₚₚ] ]\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3be66f71-03b0-4a9c-adf1-ffb5ec4ec9af",
   "metadata": {},
   "source": [
    "3.\tCalculating the Variance-Covariance Matrix: To calculate Σ, we use the following formula:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc46a269-48ad-4354-baab-21b72450cea4",
   "metadata": {},
   "source": [
    "$$\n",
    "Σ = (1 / (n-1)) * (X - \\tilde{X̄})ᵀ * (X - \\tilde{X̄})\n",
    "$$\n",
    "<br>Where:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e0fb64-d3fd-4053-995d-656a32c05980",
   "metadata": {},
   "source": [
    "<br>•\tX is the original data matrix\n",
    "<br>•\tX̄ is a matrix where each column is the mean of the corresponding column in X\n",
    "<br>•\t(X - X̄) is the centered data matrix\n",
    "<br>•\tᵀ denotes the transpose of a matrix\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bf4f7fa-4397-4226-a734-cd6b223a093a",
   "metadata": {},
   "source": [
    "4.\tProperties of the Variance-Covariance Matrix:\n",
    "<br>•\tIt is always symmetric: σᵢⱼ = σⱼᵢ\n",
    "<br>•\tThe diagonal elements (σᵢᵢ) represent the variance of the i-th variable\n",
    "<br>•\tThe off-diagonal elements (σᵢⱼ, i ≠ j) represent the covariance between the i-th and j-th variables\n",
    "5.\tInterpretation:\n",
    "<br>•\tLarge values on the diagonal indicate high variability in that particular variable.\n",
    "<br>•\tOff-diagonal elements show how variables covary with each other. Positive values indicate positive correlation, negative values indicate negative correlation, and values close to zero indicate little to no linear relationship.\n",
    "6.\tCorrelation Matrix: Sometimes, it's useful to standardize the variance-covariance matrix to get the correlation matrix. This is done by dividing each element by the product of the standard deviations of the corresponding variables:\n",
    "\n",
    "$$\n",
    "ρᵢⱼ = σᵢⱼ / (√σᵢᵢ * √σⱼⱼ)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f23231e-a812-400c-8d3b-2ba599af77a7",
   "metadata": {},
   "source": [
    "This matrix has 1's on the diagonal and correlation coefficients (between -1 and 1) on the off-diagonal elements."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66b2a635-39ea-40a1-bca3-56db86ad7e39",
   "metadata": {},
   "source": [
    "Let's consider a data matrix X (n × p), where n is the number of observations and p is the number of variables. We assume the data is centered (mean-subtracted)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be578bbb-1120-4e0d-a43f-08a4e3224319",
   "metadata": {},
   "source": [
    "We want to find a unit vector w that maximizes the variance of the projected data:\n",
    "$$\n",
    "\\begin{cases}\n",
    "        \\max_{w} (var(Xw)=w^T Σ w)\\\\\n",
    "        \\| w \\| = 1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "<br>where Σ  - is the covariance matrix of X."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83be5386-349a-414c-ae9a-6f7eb2cec63c",
   "metadata": {},
   "source": [
    "Breaking down the projections on w:\n",
    "<br>1.\tStarting with the data matrix X: X is an n × p matrix, where n is the number of observations and p is the number of variables. We assume X is centered (mean-subtracted).\n",
    "<br>2.\tProjection onto w: When we project X onto a unit vector w, we get Xw, which is an n × 1 vector.\n",
    "<br>3.\tVariance of the projection: The variance of this projection is what we want to maximize. Let's derive this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcc0b14-8dcb-4338-badd-be8030f24d29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea790371-3e01-4a0a-a4ed-5d167d02bad1",
   "metadata": {},
   "source": [
    "<br>Let's write down a Lagrangian of our optimization problem:\n",
    "$$\n",
    "L(w,\\lambda)=w^T Σ w - \\lambda(w^Tw - 1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a26427c-efcc-4e5c-9fdd-f7a32e7e92a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
